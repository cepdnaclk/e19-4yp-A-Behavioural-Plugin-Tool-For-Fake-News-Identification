# -*- coding: utf-8 -*-
"""SubRegion_Feature.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LNjXUCQy5M1kFqV7mWWyjkJt82HzaH7M

# Shared Code
"""

import pandas as pd

# Load main dataset and demonyms
df = pd.read_csv('all_validate.csv')
df_copy = df.copy()
demonyms_df = pd.read_csv('demonyms.csv', header=None)
demonym_map = dict(zip(demonyms_df[0].str.lower(), demonyms_df[1]))

"""# 1. GeoText"""

!pip install geotext

"""## With Dymonyms"""

from geotext import GeoText

def get_country_geotext_with_demonyms(text):
    text = str(text).lower()
    places = GeoText(text)
    countries = places.countries
    if countries:
        return countries[0]
    for word in text.split():
        if word in demonym_map:
            return demonym_map[word]
    return None

df_copy['predicted_country'] = df_copy['title'].apply(get_country_geotext_with_demonyms)
found = df_copy['predicted_country'].notna().sum()
print(f"GeoText + Demonyms: Found {found}/{len(df_copy)} entries")
df_copy.to_csv('all_validate_geotext.csv', index=False)
print(df_copy[df_copy['predicted_country'].notna()].sample(10)[['title', 'predicted_country']])

"""## Without Dymonyms"""

from geotext import GeoText

def get_country_geotext(text):
    places = GeoText(str(text))
    return places.countries[0] if places.countries else None

df_copy['predicted_country'] = df_copy['title'].apply(get_country_geotext)
found = df_copy['predicted_country'].notna().sum()
print(f"GeoText Only: Found {found}/{len(df_copy)} entries")
df_copy.to_csv('all_validate_geotext_nodemonym.csv', index=False)
print(df_copy[df_copy['predicted_country'].notna()].sample(10)[['title', 'predicted_country']])

"""# 2. SpacyEr"""

!pip install spacy
!python -m spacy download en_core_web_sm

"""## With Demonyms"""

import spacy

nlp = spacy.load("en_core_web_sm")

def get_country_spacy_with_demonyms(text):
    text = str(text)
    doc = nlp(text)
    for ent in doc.ents:
        if ent.label_ == "GPE":
            return ent.text
    for word in text.lower().split():
        if word in demonym_map:
            return demonym_map[word]
    return None

df_copy['predicted_country'] = df_copy['title'].apply(get_country_spacy_with_demonyms)
found = df_copy['predicted_country'].notna().sum()
print(f"spaCy + Demonyms: Found {found}/{len(df_copy)} entries")
df_copy.to_csv('all_validate_spacy.csv', index=False)
print(df_copy[df_copy['predicted_country'].notna()].sample(10)[['title', 'predicted_country']])

"""## Without Demonyms"""

import spacy

nlp = spacy.load("en_core_web_sm")

def get_country_spacy(text):
    doc = nlp(str(text))
    for ent in doc.ents:
        if ent.label_ == "GPE":
            return ent.text
    return None

df_copy['predicted_country'] = df_copy['title'].apply(get_country_spacy)
found = df_copy['predicted_country'].notna().sum()
print(f"spaCy Only: Found {found}/{len(df_copy)} entries")
df_copy.to_csv('all_validate_spacy_nodemonym.csv', index=False)
print(df_copy[df_copy['predicted_country'].notna()].sample(10)[['title', 'predicted_country']])

"""# Correlation Map

## Geotext with demonyms - but for only the entries with country encoded

### All countries together
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('all_validate_geotext.csv')

# Filter out rows where 'predicted_country' is missing or empty
df_filtered = df[df['predicted_country'].notna() & (df['predicted_country'] != '')]

# Encode countries as numbers
df_filtered['country_encoded'] = df_filtered['predicted_country'].astype('category').cat.codes

# Compute correlation
corr_matrix = df_filtered.corr(numeric_only=True)

# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap (Only Entries with Country)')
plt.show()

# Get all numeric columns except country_encoded
numeric_cols = df_filtered.select_dtypes(include='number').columns.tolist()
numeric_cols.remove('country_encoded')

# Plot scatter plots in grid (2 columns layout)
n = len(numeric_cols)
cols = 2
rows = (n + cols - 1) // cols

fig, axes = plt.subplots(rows, cols, figsize=(14, 5 * rows))
axes = axes.flatten()

for i, col in enumerate(numeric_cols):
    sns.scatterplot(x='country_encoded', y=col, data=df_filtered, ax=axes[i])
    axes[i].set_title(f'Country vs {col}')
    axes[i].set_xlabel('Encoded Country')
    axes[i].set_ylabel(col)

# Hide any empty subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""### For each country"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('all_validate_geotext.csv')

# Only keep rows with valid predicted_country
df = df[df['predicted_country'].notna() & (df['predicted_country'] != '')]

# List of features to analyse
features = ['score', 'num_comments', 'upvote_ratio', '2_way_label', '3_way_label', '6_way_label']
scatter_features = ['score', 'num_comments', 'upvote_ratio']

# Get unique countries
countries = df['predicted_country'].unique()

# Loop through each country
for country in countries:
    df_country = df[df['predicted_country'] == country]

    if len(df_country) < 10:
        continue  # Skip small groups for reliability

    print(f"\n=== Analysis for Country: {country} ===")

    # Set up the row of 4 plots (1 heatmap + 3 scatter)
    fig, axes = plt.subplots(1, 4, figsize=(24, 5))  # 1 row, 4 columns

    # --- Correlation Heatmap ---
    corr = df_country[features].corr(numeric_only=True)
    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", ax=axes[0])
    axes[0].set_title('Correlation Heatmap')

    # --- Scatter Plots ---
    for i, feature in enumerate(scatter_features):
        sns.scatterplot(data=df_country, x='2_way_label', y=feature, ax=axes[i + 1])
        axes[i + 1].set_title(f'{feature} vs 2_way_label')
        axes[i + 1].set_xlabel('Fake (0) / Real (1)')
        axes[i + 1].set_ylabel(feature)

    fig.suptitle(f'Feature Analysis for Country: {country}', fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # leave space for suptitle
    plt.show()

"""## Spacy

### All countries together
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load spaCy + demonyms file
df = pd.read_csv('all_validate_spacy.csv')

# Keep only rows with valid country predictions
df_filtered = df[df['predicted_country'].notna() & (df['predicted_country'] != '')]

# Encode countries numerically
df_filtered['country_encoded'] = df_filtered['predicted_country'].astype('category').cat.codes

# Define features
features = ['score', 'num_comments', 'upvote_ratio', '2_way_label', '3_way_label', '6_way_label']

# Compute correlation matrix
corr_matrix = df_filtered[features + ['country_encoded']].corr(numeric_only=True)

# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap (All Countries - spaCy)')
plt.tight_layout()
plt.show()

# Scatter plots: each feature vs encoded country
numeric_cols = [col for col in features if df_filtered[col].dtype != 'object']

cols = 2
rows = (len(numeric_cols) + cols - 1) // cols

fig, axes = plt.subplots(rows, cols, figsize=(14, 5 * rows))
axes = axes.flatten()

for i, col in enumerate(numeric_cols):
    sns.scatterplot(x='country_encoded', y=col, data=df_filtered, ax=axes[i])
    axes[i].set_title(f'{col} vs Encoded Country')
    axes[i].set_xlabel('Encoded Country')
    axes[i].set_ylabel(col)

# Hide any empty subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""### Countries separately"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load spaCy + demonyms file
df = pd.read_csv('all_validate_spacy.csv')

# Keep only rows with valid predicted_country
df = df[df['predicted_country'].notna() & (df['predicted_country'] != '')]

# Define features
features = ['score', 'num_comments', 'upvote_ratio', '2_way_label', '3_way_label', '6_way_label']
scatter_features = ['score', 'num_comments', 'upvote_ratio']

# Get list of countries
countries = df['predicted_country'].unique()

# Loop through each country
for country in countries:
    df_country = df[df['predicted_country'] == country]

    if len(df_country) < 10:
        continue  # Skip countries with too few entries

    print(f"\n=== Analysis for Country: {country} ===")

    # Create subplot: 1 heatmap + 3 scatter plots
    fig, axes = plt.subplots(1, 4, figsize=(24, 5))

    # Correlation heatmap
    corr = df_country[features].corr(numeric_only=True)
    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", ax=axes[0])
    axes[0].set_title('Correlation Heatmap')

    # Scatter plots: score, num_comments, upvote_ratio vs 2_way_label
    for i, feature in enumerate(scatter_features):
        sns.scatterplot(data=df_country, x='2_way_label', y=feature, ax=axes[i + 1])
        axes[i + 1].set_title(f'{feature} vs 2_way_label')
        axes[i + 1].set_xlabel('Fake (0) / Real (1)')
        axes[i + 1].set_ylabel(feature)

    # Add country name as overall title
    fig.suptitle(f'Feature Analysis for Country: {country}', fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

"""# GeoText - Sub Regions (Demonyms and SubContinent Datasets)"""

!pip install geotext

!pip install chardet

import chardet

with open('continents-map.csv', 'rb') as f:
    result = chardet.detect(f.read())
    print(result)

import pandas as pd
from geotext import GeoText
import seaborn as sns
import matplotlib.pyplot as plt

# Load main dataset
df = pd.read_csv('all_validate.csv')

# Load demonyms dataset
demonyms_df = pd.read_csv('demonyms.csv', header=None, names=['demonym', 'country'])
demonym_map = dict(zip(demonyms_df['demonym'].str.lower(), demonyms_df['country']))

# Load continents-map dataset
continent_df = pd.read_csv('continents-map.csv', encoding='ISO-8859-1')
continent_map = dict(zip(continent_df['name'], continent_df['sub-region']))

# Function to extract location and map to subregion
def get_subregion_from_title(title):
    title = str(title)
    places = GeoText(title)

    for place in places.cities + places.countries:
        country = demonym_map.get(place.lower())
        if not country:
            country = place
        subregion = continent_map.get(country)
        if subregion:
            return subregion

    for word in title.lower().split():
        country = demonym_map.get(word)
        if country:
            subregion = continent_map.get(country)
            if subregion:
                return subregion

    return None

# Apply function
df['sub_region'] = df['title'].apply(get_subregion_from_title)

# Save to new CSV
df.to_csv('all_validate_with_subregion.csv', index=False)

# Print number of rows with sub-region
valid_subregion_df = df[df['sub_region'].notna() & (df['sub_region'] != '')]
print(f"Number of entries with identified sub-regions: {len(valid_subregion_df)}")

# Print 10 random samples
print(valid_subregion_df.sample(10)[['title', 'sub_region']])

# Continue with analysis using only valid entries
df = valid_subregion_df.copy()

# Features to analyse
features = ['num_comments', 'score', 'upvote_ratio', '2_way_label', '3_way_label', '6_way_label']
scatter_features = ['num_comments', 'score', 'upvote_ratio']
subregions = df['sub_region'].unique()

# Generate plots per subregion
for region in subregions:
    df_region = df[df['sub_region'] == region]

    if len(df_region) < 10:
        continue  # Skip small sets

    print(f"\n=== Analysis for Sub-Region: {region} ===")

    fig, axes = plt.subplots(1, 4, figsize=(24, 5))
    corr = df_region[features].corr(numeric_only=True)
    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", ax=axes[0])
    axes[0].set_title('Correlation Heatmap')

    for i, feature in enumerate(scatter_features):
        sns.scatterplot(data=df_region, x='2_way_label', y=feature, ax=axes[i + 1])
        axes[i + 1].set_title(f'{feature} vs 2_way_label')
        axes[i + 1].set_xlabel('Fake (0) / Real (1)')
        axes[i + 1].set_ylabel(feature)

    fig.suptitle(f'Feature Analysis for Sub-Region: {region}', fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

"""## 1. Group-wise Distribution Plots (Bar/Count)"""

import seaborn as sns
import matplotlib.pyplot as plt

# Get sorted subregions to fix order
subregion_order = df['sub_region'].value_counts().index

# Count of fake/real per sub-region
plt.figure(figsize=(16, 6))  # wider than 12
sns.countplot(data=df, x='sub_region', hue='2_way_label', order=subregion_order)
plt.xticks(rotation=90, ha='right')  # 'ha' aligns labels properly
plt.title("Fake vs Real News Count by Sub-Region")
plt.xlabel("Sub-Region")
plt.ylabel("Count")
plt.legend(title='2-Way Label', labels=['Fake (0)', 'Real (1)'])
plt.tight_layout()
plt.show()

"""## 2. Chi-Square Test of Independence"""

import seaborn as sns
import matplotlib.pyplot as plt

# Create the contingency table
contingency_table = pd.crosstab(df['sub_region'], df['2_way_label'])

# Chi-square test
chi2, p, dof, ex = chi2_contingency(contingency_table)

# Print results
print("Chi-square test result:")
print(f"Chi2 = {chi2:.2f}, p-value = {p:.4f}, degrees of freedom = {dof}")
if p < 0.05:
    print("→ Significant association between sub-region and news label.")
else:
    print("→ No significant association found.")

# Plot heatmap
prop_table = contingency_table.div(contingency_table.sum(axis=1), axis=0)

plt.figure(figsize=(10, 6))
sns.heatmap(prop_table, annot=True, fmt='.2f', cmap='YlGnBu')
plt.title('Proportion of Fake vs Real News by Sub-Region')
plt.xlabel('2-Way Label (0 = Fake, 1 = Real)')
plt.ylabel('Sub-Region')
plt.tight_layout()
plt.show()

"""## 3. Boxplots for Feature Distributions by Sub-Region"""

plt.figure(figsize=(16, 6))

# Draw boxplot without showing outliers (set flierprops marker size to 0)
sns.boxplot(data=df_filtered, x='sub_region', y='score', hue='2_way_label',
            palette='Set2', showfliers=False)

# Overlay the points (dots) with hue colouring
sns.stripplot(data=df_filtered, x='sub_region', y='score', hue='2_way_label',
              palette='Set2', dodge=True, jitter=True, alpha=0.7, marker='o', edgecolor='gray', linewidth=0.5)

plt.title('Score Distribution by Sub-Region and Label')
plt.xlabel('Sub-Region')
plt.ylabel('Score')
plt.xticks(rotation=45)

# Remove duplicate legends
handles, labels = plt.gca().get_legend_handles_labels()
plt.legend(handles[:2], ['Fake (0)', 'Real (1)'], title='2-Way Label')

plt.tight_layout()
plt.show()

"""## 4. Train Classifiers Separately for Each Sub-Region

Split the dataset by sub-region and train classifiers (e.g., Logistic Regression, Random Forest) to compare performance.

This helps see if detection accuracy varies by sub-region, which might suggest that sub-region-specific patterns exist.
"""

from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score

# Store metrics
all_metrics = []

for region in subregions:
    df_region = df[df['sub_region'] == region]
    if len(df_region) < 50:
        continue

    X = df_region[['num_comments', 'score', 'upvote_ratio']]
    y = df_region['2_way_label']

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)
    clf = RandomForestClassifier(random_state=42)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    y_prob = clf.predict_proba(X_test)[:, 1]  # for ROC AUC

    acc = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    auc = roc_auc_score(y_test, y_prob)

    print(f"\nRegion: {region}")
    print(f"  Accuracy : {acc:.2f}")
    print(f"  Precision: {precision:.2f}")
    print(f"  Recall   : {recall:.2f}")
    print(f"  F1 Score : {f1:.2f}")
    print(f"  ROC AUC  : {auc:.2f}")

    all_metrics.append([acc, precision, recall, f1, auc])

# Overall mean of each metric
if all_metrics:
    metrics_df = pd.DataFrame(all_metrics, columns=['Accuracy', 'Precision', 'Recall', 'F1', 'ROC_AUC'])
    print("\n=== Overall Mean Metrics Across Sub-Regions ===")
    print(metrics_df.mean())
else:
    print("\nNo sub-regions had sufficient data for evaluation.")

"""## 5. Multivariate Analysis: Logistic Regression with Region"""

from sklearn.preprocessing import OrdinalEncoder

# Instead of OneHotEncoder, use OrdinalEncoder for cat_cols
preprocessor = ColumnTransformer([
    ('num', make_pipeline(SimpleImputer(strategy='mean'), StandardScaler()), num_cols),
    ('cat', OrdinalEncoder(), cat_cols)
])

pipeline = make_pipeline(preprocessor, LogisticRegression(max_iter=2000))
pipeline.fit(X_train, y_train)

# After training
coefficients = pipeline.named_steps['logisticregression'].coef_[0]
feature_names = num_cols + cat_cols  # only one 'sub_region' feature now

print("\nFeature Weights (Logistic Regression Coefficients):")
for name, coef in zip(feature_names, coefficients):
    print(f"{name}: {coef:.3f}")

"""# Spacy (Demonyms, continents-map)"""

!pip install spacy
!python m spacy download en_core_web_sm

import pandas as pd
import spacy
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.linear_model import LogisticRegression
from scipy.stats import chi2_contingency

# Load spaCy English model
nlp = spacy.load("en_core_web_sm")

# Load main dataset
df = pd.read_csv('all_validate.csv')

# Load demonyms dataset
demonyms_df = pd.read_csv('demonyms.csv', header=None, names=['demonym', 'country'])
demonym_map = dict(zip(demonyms_df['demonym'].str.lower(), demonyms_df['country']))

# Load continents-map dataset
continent_df = pd.read_csv('continents-map.csv', encoding='ISO-8859-1')
continent_map = dict(zip(continent_df['name'], continent_df['sub-region']))

# Function to extract location and map to subregion using spaCy NER
def get_subregion_from_title_spacy(title):
    title = str(title)
    doc = nlp(title)

    # Extract entities labeled as GPE or LOC
    places = [ent.text for ent in doc.ents if ent.label_ in ('GPE', 'LOC')]

    for place in places:
        country = demonym_map.get(place.lower())
        if not country:
            country = place
        subregion = continent_map.get(country)
        if subregion:
            return subregion

    # Fallback: check each word against demonym_map
    for word in title.lower().split():
        country = demonym_map.get(word)
        if country:
            subregion = continent_map.get(country)
            if subregion:
                return subregion

    return None

# Apply function
df['sub_region'] = df['title'].apply(get_subregion_from_title_spacy)

# Save to new CSV
df.to_csv('all_validate_with_subregion.csv', index=False)

# Filter entries with identified sub-regions
valid_subregion_df = df[df['sub_region'].notna() & (df['sub_region'] != '')]
print(f"Number of entries with identified sub-regions: {len(valid_subregion_df)}")

print(valid_subregion_df.sample(10)[['title', 'sub_region']])

# Continue with analysis using only valid entries
df = valid_subregion_df.copy()

# Features for analysis
features = ['num_comments', 'score', 'upvote_ratio', '2_way_label', '3_way_label', '6_way_label']
scatter_features = ['num_comments', 'score', 'upvote_ratio']
subregions = df['sub_region'].unique()

# Generate plots per subregion
for region in subregions:
    df_region = df[df['sub_region'] == region]

    if len(df_region) < 10:
        continue  # Skip small sets

    print(f"\n=== Analysis for Sub-Region: {region} ===")

    fig, axes = plt.subplots(1, 4, figsize=(24, 5))
    corr = df_region[features].corr(numeric_only=True)
    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", ax=axes[0])
    axes[0].set_title('Correlation Heatmap')

    for i, feature in enumerate(scatter_features):
        sns.scatterplot(data=df_region, x='2_way_label', y=feature, ax=axes[i + 1])
        axes[i + 1].set_title(f'{feature} vs 2_way_label')
        axes[i + 1].set_xlabel('Fake (0) / Real (1)')
        axes[i + 1].set_ylabel(feature)

    fig.suptitle(f'Feature Analysis for Sub-Region: {region}', fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

# Countplot for fake vs real per sub-region
subregion_order = df['sub_region'].value_counts().index

plt.figure(figsize=(16, 6))
sns.countplot(data=df, x='sub_region', hue='2_way_label', order=subregion_order)
plt.xticks(rotation=90, ha='right')
plt.title("Fake vs Real News Count by Sub-Region")
plt.xlabel("Sub-Region")
plt.ylabel("Count")
plt.legend(title='2-Way Label', labels=['Fake (0)', 'Real (1)'])
plt.tight_layout()
plt.show()

# Chi-square test for association
contingency_table = pd.crosstab(df['sub_region'], df['2_way_label'])
chi2, p, dof, ex = chi2_contingency(contingency_table)

print("Chi-square test result:")
print(f"Chi2 = {chi2:.2f}, p-value = {p:.4f}, degrees of freedom = {dof}")
if p < 0.05:
    print("→ Significant association between sub-region and news label.")
else:
    print("→ No significant association found.")

# Heatmap of proportions
prop_table = contingency_table.div(contingency_table.sum(axis=1), axis=0)
plt.figure(figsize=(10, 6))
sns.heatmap(prop_table, annot=True, fmt='.2f', cmap='YlGnBu')
plt.title('Proportion of Fake vs Real News by Sub-Region')
plt.xlabel('2-Way Label (0 = Fake, 1 = Real)')
plt.ylabel('Sub-Region')
plt.tight_layout()
plt.show()

# Boxplot of 'score' by sub-region and label
df_filtered = df.copy()  # (If you want to filter, do here)

plt.figure(figsize=(16, 6))
sns.boxplot(data=df_filtered, x='sub_region', y='score', hue='2_way_label',
            palette='Set2', showfliers=False)
sns.stripplot(data=df_filtered, x='sub_region', y='score', hue='2_way_label',
              palette='Set2', dodge=True, jitter=True, alpha=0.7, marker='o',
              edgecolor='gray', linewidth=0.5)

plt.title('Score Distribution by Sub-Region and Label')
plt.xlabel('Sub-Region')
plt.ylabel('Score')
plt.xticks(rotation=45)

handles, labels = plt.gca().get_legend_handles_labels()
plt.legend(handles[:2], ['Fake (0)', 'Real (1)'], title='2-Way Label')
plt.tight_layout()
plt.show()

# Classification per sub-region with RandomForest
all_metrics = []

for region in subregions:
    df_region = df[df['sub_region'] == region]
    if len(df_region) < 50:
        continue

    X = df_region[['num_comments', 'score', 'upvote_ratio']]
    y = df_region['2_way_label']

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)
    clf = RandomForestClassifier(random_state=42)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    y_prob = clf.predict_proba(X_test)[:, 1]

    acc = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    auc = roc_auc_score(y_test, y_prob)

    print(f"\nRegion: {region}")
    print(f"  Accuracy : {acc:.2f}")
    print(f"  Precision: {precision:.2f}")
    print(f"  Recall   : {recall:.2f}")
    print(f"  F1 Score : {f1:.2f}")
    print(f"  ROC AUC  : {auc:.2f}")

    all_metrics.append([acc, precision, recall, f1, auc])

if all_metrics:
    metrics_df = pd.DataFrame(all_metrics, columns=['Accuracy', 'Precision', 'Recall', 'F1', 'ROC_AUC'])
    print("\n=== Overall Mean Metrics Across Sub-Regions ===")
    print(metrics_df.mean())
else:
    print("\nNo sub-regions had sufficient data for evaluation.")

# Logistic Regression with OrdinalEncoder for sub_region
num_cols = ['num_comments', 'score', 'upvote_ratio']
cat_cols = ['sub_region']

preprocessor = ColumnTransformer([
    ('num', make_pipeline(SimpleImputer(strategy='mean'), StandardScaler()), num_cols),
    ('cat', OrdinalEncoder(), cat_cols)
])

pipeline = make_pipeline(preprocessor, LogisticRegression(max_iter=2000))

# Assuming you want to train on the whole filtered df here:
X = df[num_cols + cat_cols]
y = df['2_way_label']
pipeline.fit(X, y)

coefficients = pipeline.named_steps['logisticregression'].coef_[0]
feature_names = num_cols + cat_cols

print("\nFeature Weights (Logistic Regression Coefficients):")
for name, coef in zip(feature_names, coefficients):
    print(f"{name}: {coef:.3f}")